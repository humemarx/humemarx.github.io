<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[人体姿态估计论文——《Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields》]]></title>
    <url>%2F2018%2F04%2F22%2Fpose%2F</url>
    <content type="text"><![CDATA[介绍 多人体姿态估计的方法主要分为两大类，自顶向下（top-bottom）和自底向上(bottom-top)的方法。 自顶向下的方法是指先通过人体检测检测出每个人体框的位置，然后对每个人体框单独进行人体关节点估计，一般的RCNN都是这个思路，这类方法通常可以获得很高的精度，但是速度较慢。 而自底向上的方法是指先估计出人体的关节点，然后根据图论知识判断每个关节点归属于哪一个人，本文介绍的这篇论文就是这种方法。 论文简介 《Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields》这篇论文是在《Convolutional Pose Machines》这篇论文的基础上对多人的一个扩展，主要思路是先估计出人体关节点，然后利用PAFs(Part Affinity Fields)来对关节点分组，论文的主要流程图如下： 图中F是图像feature, Branch1是人体关节点HeatMap的回归过程，Branch2是PAFs的回归过程，每个stage的输入都是上一个stage的输出和F的concat。每个stage的输出如下： 在这篇论文中作者使用了L2 loss来估计预测误差，同时由于在实际情况中，数据中的有一部分人体可能并没有进行标注，所以会在loss上加上一个权重，当图像中的某个位置没有标注时，该值为0；否则为1。 作者在论文中说道，如果没有这个二值mask会导致在训练过程中对真实的正样本进行过度惩罚，因为我们会将没有标记的正样本当成是负样本进行学习，正样本的学习会变得困难。 最终的Loss计算如下： 上式中： \({\bf S}^*_j\)是第\(j\)个关节点的groundtruth confidence map，是基于标注点生成的高斯分布图， \({\bf L}^*_c\)是groundtruth part affinity vector field，是基于关节点之间计算生成的向量场。一般情况下我们用余弦距离来衡量两个向量的距离，但归一化条件下的欧式距离和余弦距离是等价的。 Confidence Maps for Part Detection 每一个confidence map都是用来表示每个关节点在某个像素位置上存在的置信度，我们用高斯分布来定义，在标注点的置信度为1，在其周围根据像素距离呈高斯分布扩散，总体上就是距离越远，置信度越低；距离越近，置信度越高。\({\bf S}^*_j\)的计算公式如下： 上式中： \({\bf x}_{j,k}\)是第\(k\)个人的第\(j\)个关节点的真实标注位置 \(\sigma\)是高斯方差，用来控制扩散速度 如果是单个人，那么在每个confidence map应该都只有一个峰值；如果是多个人，在confidence map重叠的区域，我们取多个值中的最大值作为最后的值，计算公式如下： 在每个关节点的confidence map图中会有多个峰值点，如下图所示： Part Affinity Fields for Part Association 在检测出了多个人体关节点后，将每个关节点和不同人对应起来有多种方法，一种最为直观的方法是就是在每一对关节点之间在多预测一个中间节点，然后通过判断一对候选的关节点和中间节点是否在一条直线上来确定它们是否属于同一个人。 但是这种方法在多人密集的时候会出现较大程度的误判，一方面由于它只考虑了关节点的位置，没有考虑方向，另一方面，它将一个关节区域压缩为了一个单独的点。 上图(b)中,黑色的线是正确的匹配，绿色的线就是错误的匹配 而Part Affinity Fields是一种保留关节点区域位置和方向信息的方法，如上图(c)所示,paf的计算如下公式： 上式中： \({\bf v} = \frac{({\bf x}_{j_2,k}-{\bf x}_{j_1,k})}{||{\bf x}_{j_2,k}-{\bf x}_{j_1,k}||_2}\) 是在关节方向的单位向量，位置\({\bf p}\)需要满足以下条件，即位于关节区域内： 上式中： \(\sigma_l\)是关节点的宽度 \(l_{c,k}\)是关节点的长度 在多人情况，paf对于多人重叠区域的的取值跟confidence map稍有不同，这里是通过取平均值而不是最大值，公式如下： 人体骨骼连接 在通过网络预测出关节点的置信度点图和关节向量场之后，要如何连接图像中对应人的关节点呢？ 基本思路是： 1.对于置信度点图，在每一个小范围的区域内（例如3x3或者5x5等）寻找出极大值的位置，该点作为候选的关节点位置 2.计算两个关节点之间的区域的每个向量和对应位置的paf的余弦相似度之和，该值作为关节点图匹配的边的权值，在所有可能存在的连接中找出权值最大的那个，图匹配过程如下图所示： 整个权值的计算过程如下： 至此，我们通过不断的匹配最好的关节点，可以将每个人的骨骼全部连接完成。 总结 基于confidence map和paf的方法目前来看是一种性价比很高的多人姿态估计算法，它比大多数的top-down的方法要快，同时效果还能保持一定的算法性能。 参考文献 openpose]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>人体姿态估计</tag>
        <tag>CMU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(一)线性回归]]></title>
    <url>%2F2017%2F03%2F18%2Flinear%2F</url>
    <content type="text"><![CDATA[介绍 线性回归是机器学习中的重要算法之一，其主要目的就是建立一个线性方程来预测目标值.线性回归所求解的是回归方程的回归系数. 举个例子，我们现在有一组数据如下所示： \(x\) 3.30 4.40 5.50 6.71 6.93 4.17 9.78 6.18 7.59 2.17 7.04 10.79 5.31 7.99 5.65 9.27 3.10 \(y\) 1.70 2.76 2.09 3.19 1.69 1.58 3.37 2.60 2.53 1.22 2.83 3.47 1.65 2.90 2.42 2.94 1.30 数据归一化代码以及最终分布图如下所示： 12345678910111213141516171819202122232425262728'''Created on 2015-8-15@author: marxwebsite: http://marxrobot.com'''import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns# sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)sns.set(style="ticks")sns.set(color_codes=True)def normalize_mean(arr): arr_mean = np.mean(arr) arr_std = np.std(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_mean),arr_std),arr) return np.array(arr_out)if __name__ == '__main__': trX = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]) trY = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]) ## normalize train_X = normalize_mean(trX) train_Y = normalize_mean(trY) plt.plot(train_X, train_Y, 'ro',color="b", label="Normalize data") plt.legend() plt.show() 线性回归 线性回归的一般问题就是，针对给出的数据，拟合出一个能够较为准确预测出输出结果的线性模型，模型一般如下： \[ f(x) = wx+b \tag {1} \] \[J(w,b) = \frac{1}{2}\sum_{i=1}^n(y_i-f(x_i))^2 = \frac{1}{2}\sum_{i=1}^n(y_i-wx_i-b)^2 \tag{2}\] 上式中: \(f(x)\)是预测值 \(y_i\)是真实值 \(J(w,b)\)是代价函数 \(x_i\)是输入值 \(w\)和\(b\)是回归方程需要求解的参数 \(J(w,b)\)表示了预测结果和真实结果之间的误差，\(J(w,b)\)越小表明预测结果越接近真实结果，因此我们希望找到一组\(w\)和\(b\)使得\(J(w,b)\)能够最小，因此对\(w\)，\(b\)的求解问题就变成了一个如下所示的泛函能量最小化的问题： \[ \hat{(w,b)} = \mathop{\arg\min}\limits_{w,b}(J(w,b)) \tag{3} \] 根据公式\({2}\)分别求\(J(w,b)\)关于\(w\)和\(b\)的偏导数： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{4} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) \tag{5} \] 最小二乘法 对于公式\({(3)}\)中\(J(w,b)\)的最小化问题，我们可以将其看作是自变量为\(w\),\(b\),因变量为\(J(w,b)\)的函数，这样最小化的问题就变成了求\(J(w,b)\)极值的问题，根据数学知识我们知道，函数的极值点为偏导数为\(0\)的点，因此： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) = 0 \tag{6} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) = 0 \tag{7} \] 求解方程组得到： \[ w = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2} \tag{8} \] \[ b = \mathop{y}\limits^--w\mathop{x}\limits^- \tag{9} \] 上式中： \(\mathop{x}\limits^-=\frac{1}{n}\sum_{i=1}^nx_i\):表示\(x\)的均值 \(\mathop{y}\limits^-=\frac{1}{n}\sum_{i=1}^ny_i\):表示\(y\)的均值 根据样本的方差和协方差公式我们有： \[ var(x) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2}{n-1} \tag{10}\] \[ cov(x,y) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{n-1} \tag{11}\] 方差\(var(x)\)是用来衡量样本分散程度的，方差越小，样本越集中；方差越大，样本越分散 而协方差是用来衡量两个变量的线性相关性，协方差为正，说明两个变量是线性正相关；协方差为负，说明两个变量是线性负相关；协方差为\(0\)，说明两个变量没有线性相关性. 故式\((8)\)可表示为： \[ w = \frac{conv(x,y)}{var(x)} \tag{12} \] 最小二乘法的优势在于计算简单，快速，并且找到的估计参数是全局极小值；缺点是对于异常值极其敏感. 对上述数据利用最小二乘法拟合得到的曲线： 12345678def least_squares(train_X, train_Y): mean_x = np.mean(train_X) mean_y = np.mean(train_Y) var_x = np.var(train_X,ddof=1) cov_xy = np.cov(train_X,train_Y)[0][1] w = cov_xy/var_x b = mean_y-w*mean_x return w,b 梯度下降法 梯度下降是一种最优化算法，对一个函数来说，梯度不仅表示某个向量的偏导数，同时还代表了该向量的方向，在这个方向上，函数增加得最快，在相反的方向上，函数减小得最快。因此为了最小化目标函数\(J(w,b)\),我们可以通过不断迭代的方法，使\(w\)和\(b\)沿着梯度下降的方向进行移动，逐渐逼近极小值. \[ w^{\tau+1} := w^\tau - \alpha\frac{\partial J(w,b)}{\partial w} = w^\tau + \alpha\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{13} \] \[ b^{\tau+1} := b^\tau - \alpha\frac{\partial J(w,b)}{\partial b} = b^\tau + \alpha\sum_{i=1}^n(y_i-wx_i-b) \tag{14} \] 上式中 \(\tau\)是迭代次数 \(\alpha\)是学习率即每次参数移动的步长，学习率太大则可能会错过极小值，太小则会导致收敛速度太慢 \(w^\tau\)和\(b^\tau\)是第\(\tau\)次的参数 \(w^{\tau+1}\)和\(b^{\tau+1}\)是第\({\tau+1}\)次的参数 12345678910111213141516171819def BGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: delta_w = np.mean(train_X*(train_Y-train_X*w-b)) delta_b = np.mean(train_Y-train_X*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'BGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costs 随机梯度下降法 由式\({(13)}\)和\({(14)}\)可知，梯度下降法的每次参数更新，都需要计算整个数据集,因此运算效率是极低的.而随机梯度下降法的每次更新，都是对数据集中的一个样本计算其代价函数，然后求偏导更新模型参数： \[ w^{\tau+1} := w^\tau + {\alpha}x_i(y_i-wx_i-b) \tag{15} \] \[ b^{\tau+1} := b^\tau + {\alpha}(y_i-wx_i-b) \tag{16} \] 123456789101112131415161718192021def SGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: index = (grad_iter-1)%(train_X.size-1) delta_w = train_X[index]*(train_Y[index]-train_X[index]*w-b) delta_b = train_Y[index]-train_X[index]*w-b ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'SGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costs 小批量梯度下降法 每次只用一个样本对模型参数进行更新会导致迭代过程中模型参数的变化会很剧烈，同时目标函数在寻找极小值的过程中会有较大震荡.小批量梯度下降法的每次更新，都会对数据集中的一小批样本计算其代价函数，然后求偏导更新模型参数. \[ w^{\tau+1} := w^\tau + {\alpha}\sum_{i=1}^mx_i(y_i-wx_i-b) \tag{17} \] \[ b^{\tau+1} := b^\tau + {\alpha}\sum_{i=1}^m(y_i-wx_i-b) \tag{18} \] 其中： \(m{\subset}n\), \(m\)是\(n\)的子集 123456789101112131415161718192021222324def MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] idx_s = 0 idx_e = (idx_s+mini_batch_size) while grad_iter&lt;=max_iter: delta_w = np.mean(train_X[idx_s:idx_e]*(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b)) delta_b = np.mean(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'MBGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 idx_s = idx_e%(train_X.size-1) idx_e = (idx_s+mini_batch_size) return w,b,iters,costs 对于文章开头的数据利用不同的梯度下降法拟合的完整代码以及得到的曲线为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133'''Created on 2015-8-15@author: marxwebsite: http://marxrobot.com'''import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns# sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)sns.set(style="ticks")sns.set(color_codes=True)def normalize_mean(arr): arr_mean = np.mean(arr) arr_std = np.std(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_mean),arr_std),arr) return np.array(arr_out)def normalize_max(arr): arr_min = np.min(arr) arr_max = np.max(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_min),np.subtract(arr_max,arr_min)),arr) return np.array(arr_out)def least_squares(train_X, train_Y): mean_x = np.mean(train_X) mean_y = np.mean(train_Y) var_x = np.var(train_X,ddof=1) cov_xy = np.cov(train_X,train_Y)[0][1] w = cov_xy/var_x b = mean_y-w*mean_x return w,bdef BGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: delta_w = np.mean(train_X*(train_Y-train_X*w-b)) delta_b = np.mean(train_Y-train_X*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'BGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costsdef SGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: index = (grad_iter-1)%(train_X.size-1) delta_w = train_X[index]*(train_Y[index]-train_X[index]*w-b) delta_b = train_Y[index]-train_X[index]*w-b ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'SGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costsdef MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] idx_s = 0 idx_e = (idx_s+mini_batch_size) while grad_iter&lt;=max_iter: delta_w = np.mean(train_X[idx_s:idx_e]*(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b)) delta_b = np.mean(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'MBGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 idx_s = idx_e%(train_X.size-1) idx_e = (idx_s+mini_batch_size) return w,b,iters,costsif __name__ == '__main__': trX = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]) trY = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]) ## normalize train_X = normalize_mean(trX) train_Y = normalize_mean(trY) learning_rate = 0.01 max_iter = 1000 mini_batch_size = 8 w1,b1 = least_squares(train_X,train_Y) w2,b2,iters2,costs2 = BGD(train_X,train_Y,learning_rate,max_iter) w3,b3,iters3,costs3 = SGD(train_X,train_Y,learning_rate,max_iter) w4,b4,iters4,costs4 = MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size) plt.figure(1) plt.plot(train_X, train_Y, 'ro',color="b", label="Normalize data") plt.plot(train_X, w1*train_X+b1, 'r',linewidth=1.0,label="Least squares") plt.plot(train_X, w2*train_X+b2, 'y',linewidth=1.0,label="BGD") plt.plot(train_X, w3*train_X+b3, 'm',linewidth=1.0,label="SGD") plt.plot(train_X, w4*train_X+b4, 'c',linewidth=1.0,label="MBGD") plt.legend() plt.figure(2) plt.plot(iters2, costs2, 'y--',linewidth=1.0,label="BGD") plt.plot(iters3, costs3, 'm--',linewidth=1.0,label="SGD") plt.plot(iters4, costs4, 'c--',linewidth=1.0,label="MBGD") plt.legend() plt.show() 下图为不同梯度下降优化算法的代价函数的变化： 由上图可以看出在小数据集上几种优化算法的效果差不多,跟最小二乘法得到的结果并无太大区别. 总结 线性回归是机器学习算法中最为基础和简单的，确立了\(方法=模型+策略+算法\)的基本思路 模型，指的是样本空间之间的一种映射关系，而模型的假设空间是所有这种映射关系的集合，如线性映射，二次映射等 策略，指的是我们是通过什么准则来学习或者选择模型集合中最优的，如经验风险和结构风险最小化 算法，指的是通过什么方法去实现我们这种策略，如SGD,Adam等 线性回归中用到的各种概念，方法在其他各种更为复杂的回归方法中都是适用的，每种机器学习算法只不过是或者模型不同，或者是策略不同，或者是算法不同 参考文献 斯坦福机器学习课程 维基百科-线性回归 Python Numpy]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>线性回归</tag>
        <tag>梯度下降法</tag>
      </tags>
  </entry>
</search>
