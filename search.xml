<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习(一)线性回归]]></title>
    <url>%2F2015%2F08%2F18%2Flinear%2F</url>
    <content type="text"><![CDATA[介绍 线性回归是机器学习中的重要算法之一，其主要目的就是建立一个线性方程来预测目标值.线性回归所求解的是回归方程的回归系数. 举个例子，我们现在有一组数据如下所示： \(x\) 3.30 4.40 5.50 6.71 6.93 4.17 9.78 6.18 7.59 2.17 7.04 10.79 5.31 7.99 5.65 9.27 3.10 \(y\) 1.70 2.76 2.09 3.19 1.69 1.58 3.37 2.60 2.53 1.22 2.83 3.47 1.65 2.90 2.42 2.94 1.30 数据归一化代码以及最终分布图如下所示： 12345678910111213141516171819202122232425262728'''Created on 2015-8-15@author: marxwebsite: http://marxrobot.com'''import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns# sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)sns.set(style="ticks")sns.set(color_codes=True)def normalize_mean(arr): arr_mean = np.mean(arr) arr_std = np.std(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_mean),arr_std),arr) return np.array(arr_out)if __name__ == '__main__': trX = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]) trY = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]) ## normalize train_X = normalize_mean(trX) train_Y = normalize_mean(trY) plt.plot(train_X, train_Y, 'ro',color="b", label="Normalize data") plt.legend() plt.show() 线性回归 线性回归的一般问题就是，针对给出的数据，拟合出一个能够较为准确预测出输出结果的线性模型，模型一般如下： \[ f(x) = wx+b \tag {1} \] \[J(w,b) = \frac{1}{2}\sum_{i=1}^n(y_i-f(x_i))^2 = \frac{1}{2}\sum_{i=1}^n(y_i-wx_i-b)^2 \tag{2}\] 上式中: \(f(x)\)是预测值 \(y_i\)是真实值 \(J(w,b)\)是代价函数 \(x_i\)是输入值 \(w\)和\(b\)是回归方程需要求解的参数 \(J(w,b)\)表示了预测结果和真实结果之间的误差，\(J(w,b)\)越小表明预测结果越接近真实结果，因此我们希望找到一组\(w\)和\(b\)使得\(J(w,b)\)能够最小，因此对\(w\)，\(b\)的求解问题就变成了一个如下所示的泛函能量最小化的问题： \[ \hat{(w,b)} = \mathop{\arg\min}\limits_{w,b}(J(w,b)) \tag{3} \] 根据公式\({2}\)分别求\(J(w,b)\)关于\(w\)和\(b\)的偏导数： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{4} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) \tag{5} \] 最小二乘法 对于公式\({(3)}\)中\(J(w,b)\)的最小化问题，我们可以将其看作是自变量为\(w\),\(b\),因变量为\(J(w,b)\)的函数，这样最小化的问题就变成了求\(J(w,b)\)极值的问题，根据数学知识我们知道，函数的极值点为偏导数为\(0\)的点，因此： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) = 0 \tag{6} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) = 0 \tag{7} \] 求解方程组得到： \[ w = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2} \tag{8} \] \[ b = \mathop{y}\limits^--w\mathop{x}\limits^- \tag{9} \] 上式中： \(\mathop{x}\limits^-=\frac{1}{n}\sum_{i=1}^nx_i\):表示\(x\)的均值 \(\mathop{y}\limits^-=\frac{1}{n}\sum_{i=1}^ny_i\):表示\(y\)的均值 根据样本的方差和协方差公式我们有： \[ var(x) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2}{n-1} \tag{10}\] \[ cov(x,y) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{n-1} \tag{11}\] 方差\(var(x)\)是用来衡量样本分散程度的，方差越小，样本越集中；方差越大，样本越分散 而协方差是用来衡量两个变量的线性相关性，协方差为正，说明两个变量是线性正相关；协方差为负，说明两个变量是线性负相关；协方差为\(0\)，说明两个变量没有线性相关性. 故式\((8)\)可表示为： \[ w = \frac{conv(x,y)}{var(x)} \tag{12} \] 最小二乘法的优势在于计算简单，快速，并且找到的估计参数是全局极小值；缺点是对于异常值极其敏感. 对上述数据利用最小二乘法拟合得到的曲线： 12345678def least_squares(train_X, train_Y): mean_x = np.mean(train_X) mean_y = np.mean(train_Y) var_x = np.var(train_X,ddof=1) cov_xy = np.cov(train_X,train_Y)[0][1] w = cov_xy/var_x b = mean_y-w*mean_x return w,b 梯度下降法 梯度下降是一种最优化算法，对一个函数来说，梯度不仅表示某个向量的偏导数，同时还代表了该向量的方向，在这个方向上，函数增加得最快，在相反的方向上，函数减小得最快。因此为了最小化目标函数\(J(w,b)\),我们可以通过不断迭代的方法，使\(w\)和\(b\)沿着梯度下降的方向进行移动，逐渐逼近极小值. \[ w^{\tau+1} := w^\tau - \alpha\frac{\partial J(w,b)}{\partial w} = w^\tau + \alpha\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{13} \] \[ b^{\tau+1} := b^\tau - \alpha\frac{\partial J(w,b)}{\partial b} = b^\tau + \alpha\sum_{i=1}^n(y_i-wx_i-b) \tag{14} \] 上式中 \(\tau\)是迭代次数 \(\alpha\)是学习率即每次参数移动的步长，学习率太大则可能会错过极小值，太小则会导致收敛速度太慢 \(w^\tau\)和\(b^\tau\)是第\(\tau\)次的参数 \(w^{\tau+1}\)和\(b^{\tau+1}\)是第\({\tau+1}\)次的参数 12345678910111213141516171819def BGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: delta_w = np.mean(train_X*(train_Y-train_X*w-b)) delta_b = np.mean(train_Y-train_X*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'BGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costs 随机梯度下降法 由式\({(13)}\)和\({(14)}\)可知，梯度下降法的每次参数更新，都需要计算整个数据集,因此运算效率是极低的.而随机梯度下降法的每次更新，都是对数据集中的一个样本计算其代价函数，然后求偏导更新模型参数： \[ w^{\tau+1} := w^\tau + {\alpha}x_i(y_i-wx_i-b) \tag{15} \] \[ b^{\tau+1} := b^\tau + {\alpha}(y_i-wx_i-b) \tag{16} \] 123456789101112131415161718192021def SGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: index = (grad_iter-1)%(train_X.size-1) delta_w = train_X[index]*(train_Y[index]-train_X[index]*w-b) delta_b = train_Y[index]-train_X[index]*w-b ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'SGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costs 小批量梯度下降法 每次只用一个样本对模型参数进行更新会导致迭代过程中模型参数的变化会很剧烈，同时目标函数在寻找极小值的过程中会有较大震荡.小批量梯度下降法的每次更新，都会对数据集中的一小批样本计算其代价函数，然后求偏导更新模型参数. \[ w^{\tau+1} := w^\tau + {\alpha}\sum_{i=1}^mx_i(y_i-wx_i-b) \tag{17} \] \[ b^{\tau+1} := b^\tau + {\alpha}\sum_{i=1}^m(y_i-wx_i-b) \tag{18} \] 其中： \(m{\subset}n\), \(m\)是\(n\)的子集 123456789101112131415161718192021222324def MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] idx_s = 0 idx_e = (idx_s+mini_batch_size) while grad_iter&lt;=max_iter: delta_w = np.mean(train_X[idx_s:idx_e]*(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b)) delta_b = np.mean(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'MBGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 idx_s = idx_e%(train_X.size-1) idx_e = (idx_s+mini_batch_size) return w,b,iters,costs 对于文章开头的数据利用不同的梯度下降法拟合的完整代码以及得到的曲线为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133'''Created on 2015-8-15@author: marxwebsite: http://marxrobot.com'''import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns# sns.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=False, rc=None)sns.set(style="ticks")sns.set(color_codes=True)def normalize_mean(arr): arr_mean = np.mean(arr) arr_std = np.std(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_mean),arr_std),arr) return np.array(arr_out)def normalize_max(arr): arr_min = np.min(arr) arr_max = np.max(arr) arr_out = map(lambda x:np.divide(np.subtract(x,arr_min),np.subtract(arr_max,arr_min)),arr) return np.array(arr_out)def least_squares(train_X, train_Y): mean_x = np.mean(train_X) mean_y = np.mean(train_Y) var_x = np.var(train_X,ddof=1) cov_xy = np.cov(train_X,train_Y)[0][1] w = cov_xy/var_x b = mean_y-w*mean_x return w,bdef BGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: delta_w = np.mean(train_X*(train_Y-train_X*w-b)) delta_b = np.mean(train_Y-train_X*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'BGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costsdef SGD(train_X,train_Y,learning_rate,max_iter): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] while grad_iter&lt;=max_iter: index = (grad_iter-1)%(train_X.size-1) delta_w = train_X[index]*(train_Y[index]-train_X[index]*w-b) delta_b = train_Y[index]-train_X[index]*w-b ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'SGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 return w,b,iters,costsdef MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size): ## init paramters random w,b = np.random.randn(),np.random.randn() grad_iter = 1 iters = [] costs = [] idx_s = 0 idx_e = (idx_s+mini_batch_size) while grad_iter&lt;=max_iter: delta_w = np.mean(train_X[idx_s:idx_e]*(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b)) delta_b = np.mean(train_Y[idx_s:idx_e]-train_X[idx_s:idx_e]*w-b) ## update the paramter w += learning_rate*delta_w b += learning_rate*delta_b cost = 0.5*np.mean((train_Y-train_X*w-b)**2) print 'MBGD---&gt;iter:',grad_iter,'cost:',cost iters.append(grad_iter) costs.append(cost) grad_iter += 1 idx_s = idx_e%(train_X.size-1) idx_e = (idx_s+mini_batch_size) return w,b,iters,costsif __name__ == '__main__': trX = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1]) trY = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3]) ## normalize train_X = normalize_mean(trX) train_Y = normalize_mean(trY) learning_rate = 0.01 max_iter = 1000 mini_batch_size = 8 w1,b1 = least_squares(train_X,train_Y) w2,b2,iters2,costs2 = BGD(train_X,train_Y,learning_rate,max_iter) w3,b3,iters3,costs3 = SGD(train_X,train_Y,learning_rate,max_iter) w4,b4,iters4,costs4 = MBGD(train_X,train_Y,learning_rate,max_iter,mini_batch_size) plt.figure(1) plt.plot(train_X, train_Y, 'ro',color="b", label="Normalize data") plt.plot(train_X, w1*train_X+b1, 'r',linewidth=1.0,label="Least squares") plt.plot(train_X, w2*train_X+b2, 'y',linewidth=1.0,label="BGD") plt.plot(train_X, w3*train_X+b3, 'm',linewidth=1.0,label="SGD") plt.plot(train_X, w4*train_X+b4, 'c',linewidth=1.0,label="MBGD") plt.legend() plt.figure(2) plt.plot(iters2, costs2, 'y--',linewidth=1.0,label="BGD") plt.plot(iters3, costs3, 'm--',linewidth=1.0,label="SGD") plt.plot(iters4, costs4, 'c--',linewidth=1.0,label="MBGD") plt.legend() plt.show() 下图为不同梯度下降优化算法的代价函数的变化： 由上图可以看出在小数据集上几种优化算法的效果差不多,跟最小二乘法得到的结果并无太大区别. 总结 线性回归是机器学习算法中最为基础和简单的，确立了\(方法=模型+策略+算法\)的基本思路 模型，指的是样本空间之间的一种映射关系，而模型的假设空间是所有这种映射关系的集合，如线性映射，二次映射等 策略，指的是我们是通过什么准则来学习或者选择模型集合中最优的，如经验风险和结构风险最小化 算法，指的是通过什么方法去实现我们这种策略，如SGD,Adam等 线性回归中用到的各种概念，方法在其他各种更为复杂的回归方法中都是适用的，每种机器学习算法只不过是或者模型不同，或者是策略不同，或者是算法不同 参考文献 斯坦福机器学习课程 维基百科-线性回归 Python Numpy]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>线性回归</tag>
        <tag>梯度下降法</tag>
      </tags>
  </entry>
</search>
