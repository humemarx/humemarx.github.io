<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习(一)线性回归]]></title>
    <url>%2F2015%2F08%2F18%2Flinear%2F</url>
    <content type="text"><![CDATA[介绍 线性回归是机器学习中的重要算法之一，其主要目的就是建立一个线性方程来预测目标值.线性回归所求解的是回归方程的回归系数. 举个例子，我们现在有一组数据如下所示： \(x\) \(3.30\) \(4.40\) \(5.50\) \(6.71\) \(6.93\) \(4.17\) \(9.78\) \(6.18\) \(7.59\) \(2.17\) \(7.04\) \(10.79\) \(5.31\) \(7.99\) \(5.65\) \(9.27\) \(3.10\) \(y\) \(1.70\) \(2.76\) \(2.09\) \(3.19\) \(1.69\) \(1.58\) \(3.37\) \(2.60\) \(2.53\) \(1.22\) \(2.83\) \(3.47\) \(1.65\) \(2.90\) \(2.42\) \(2.94\) \(1.30\) 其原始和归一化后的数据分布图如下所示： 线性回归 线性回归的一般问题就是，针对给出的数据，拟合出一个能够较为准确预测出输出结果的线性模型，模型一般如下： \[ f(x) = wx+b \tag {1} \] \[J(w,b) = \frac{1}{2}\sum_{i=1}^n(y_i-f(x_i))^2 = \frac{1}{2}\sum_{i=1}^n(y_i-wx_i-b)^2 \tag{2}\] 上式中: \(f(x)\)是预测值 \(y_i\)是真实值 \(J(w,b)\)是代价函数 \(x_i\)是输入值 \(w\)和\(b\)是回归方程需要求解的参数 \(J(w,b)\)表示了预测结果和真实结果之间的误差，\(J(w,b)\)越小表明预测结果越接近真实结果，因此我们希望找到一组\(w\)和\(b\)使得\(J(w,b)\)能够最小，因此对\(w\)，\(b\)的求解问题就变成了一个如下所示的泛函能量最小化的问题： \[ \hat{(w,b)} = \mathop{\arg\min}\limits_{w,b}(J(w,b)) \tag{3} \] 根据公式\({2}\)分别求\(J(w,b)\)关于\(w\)和\(b\)的偏导数： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{4} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) \tag{5} \] 最小二乘法 对于公式\({(3)}\)中\(J(w,b)\)的最小化问题，我们可以将其看作是自变量为\(w\),\(b\),因变量为\(J(w,b)\)的函数，这样最小化的问题就变成了求\(J(w,b)\)极值的问题，根据数学知识我们知道，函数的极值点为偏导数为\(0\)的点，因此： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) = 0 \tag{6} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) = 0 \tag{7} \] 求解方程组得到： \[ w = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2} \tag{8} \] \[ b = \mathop{y}\limits^--w\mathop{x}\limits^- \tag{9} \] 上式中： \(\mathop{x}\limits^-=\frac{1}{n}\sum_{i=1}^nx_i\):表示\(x\)的均值 \(\mathop{y}\limits^-=\frac{1}{n}\sum_{i=1}^ny_i\):表示\(y\)的均值 根据样本的方差和协方差公式我们有： \[ var(x) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2}{n-1} \tag{10}\] \[ cov(x,y) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{n-1} \tag{11}\] 方差\(var(x)\)是用来衡量样本分散程度的，方差越小，样本越集中；方差越大，样本越分散 而协方差是用来衡量两个变量的线性相关性，协方差为正，说明两个变量是线性正相关；协方差为负，说明两个变量是线性负相关；协方差为\(0\)，说明两个变量没有线性相关性. 故式\((8)\)可表示为： \[ w = \frac{conv(x,y)}{var(x)} \tag{12} \] 最小二乘法的优势在于计算简单，快速，并且找到的估计参数是全局极小值；缺点是对于异常值极其敏感. 对上述数据利用最小二乘法拟合得到的曲线： 梯度下降法 梯度下降是一种最优化算法，对一个函数来说，梯度不仅表示某个向量的偏导数，同时还代表了该向量的方向，在这个方向上，函数增加得最快，在相反的方向上，函数减小得最快。因此为了最小化目标函数\(J(w,b)\),我们可以通过不断迭代的方法，使\(w\)和\(b\)沿着梯度下降的方向进行移动，逐渐逼近极小值. \[ w^{\tau+1} := w^\tau - \alpha\frac{\partial J(w,b)}{\partial w} = w^\tau + \alpha\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{13} \] \[ b^{\tau+1} := b^\tau - \alpha\frac{\partial J(w,b)}{\partial b} = b^\tau + \alpha\sum_{i=1}^n(y_i-wx_i-b) \tag{14} \] 上式中 \(\tau\)是迭代次数 \(\alpha\)是学习率即每次参数移动的步长，学习率太大则可能会错过极小值，太小则会导致收敛速度太慢 \(w^\tau\)和\(b^\tau\)是第\(\tau\)次的参数 \(w^{\tau+1}\)和\(b^{\tau+1}\)是第\({\tau+1}\)次的参数 随机梯度下降法 由式\({(13)}\)和\({(14)}\)可知，梯度下降法的每次参数更新，都需要计算整个数据集,因此运算效率是极低的.而随机梯度下降法的每次更新，都是对数据集中的一个样本计算其代价函数，然后求偏导更新模型参数： \[ w^{\tau+1} := w^\tau + {\alpha}x_i(y_i-wx_i-b) \tag{15} \] \[ b^{\tau+1} := b^\tau + {\alpha}(y_i-wx_i-b) \tag{16} \] 小批量梯度下降法 每次只用一个样本对模型参数进行更新会导致迭代过程中模型参数的变化会很剧烈，同时目标函数在寻找极小值的过程中会有较大震荡.小批量梯度下降法的每次更新，都会对数据集中的一小批样本计算其代价函数，然后求偏导更新模型参数. \[ w^{\tau+1} := w^\tau + {\alpha}\sum_{i=1}^mx_i(y_i-wx_i-b) \tag{17} \] \[ b^{\tau+1} := b^\tau + {\alpha}\sum_{i=1}^m(y_i-wx_i-b) \tag{18} \] 其中： \(m{\subset}n\), \(m\)是\(n\)的子集 梯度下降优化算法 梯度下降法中参数更新策略还有一些其他的优化方法，如动量法，Adagrad，Adadelta，Adam等等 对于文章开头的数据利用两种梯度下降法拟合得到的曲线为： 下图为不同梯度下降优化算法的代价函数的变化： 由上图可以看出Adam相对于SGD会收敛的更快，并且代价函数的震荡相对来说更小. 总结 线性回归是机器学习算法中最为基础和简单的，确立了\(方法=模型+策略+算法\)的基本思路 模型，指的是样本空间之间的一种映射关系，而模型的假设空间是所有这种映射关系的集合，如线性映射，二次映射等 策略，指的是我们是通过什么准则来学习或者选择模型集合中最优的，如经验风险和结构风险最小化 算法，指的是通过什么方法去实现我们这种策略，如SGD,Adam等 线性回归中用到的各种概念，方法在其他各种更为复杂的回归方法中都是适用的，每种机器学习算法只不过是或者模型不同，或者是策略不同，或者是算法不同]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>线性回归</tag>
        <tag>梯度下降法</tag>
      </tags>
  </entry>
</search>
