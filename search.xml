<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习(五)反向传播算法——BP]]></title>
    <url>%2F2017%2F09%2F27%2Fbp%2F</url>
    <content type="text"><![CDATA[反向传播（Backpropagation，BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法计算对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。 反向传播要求有对每个输入值想得到的已知输出，来计算损失函数梯度。因此，它通常被认为是一种监督式学习方法，虽然它也用在一些无监督网络（如自动编码器）中。它是多层前馈网络的Delta规则的推广，可以用链式法则对每层迭代计算梯度。反向传播要求人工神经元（或“节点”）的激励函数可微。 TODO]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习算法</tag>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习(四)生成对抗神经网络——GAN]]></title>
    <url>%2F2017%2F09%2F27%2Fgan%2F</url>
    <content type="text"><![CDATA[生成对抗网络（Generative Adversarial Network，GAN）是非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。他由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。生成对抗网络主要用于生成以假乱真的文字，图像等. TODO]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习算法</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习(三)递归神经网络——RNN]]></title>
    <url>%2F2017%2F09%2F27%2Frnn%2F</url>
    <content type="text"><![CDATA[递归神经网络（recurrent neural network，RNN）的神经元间连接构成矩阵，单纯递归神经网络因为无法处理随着递归，权重指数级爆炸或消失的问题（Vanishing gradient problem），难以捕捉长期时间关联；而结合不同的LSTM可以很好解决这个问题。 TODO]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习算法</tag>
        <tag>rnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习(二)卷积神经网络——CNN]]></title>
    <url>%2F2017%2F09%2F27%2Fcnn%2F</url>
    <content type="text"><![CDATA[卷积神经网路（Convolutional Neural Network, CNN）是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于图像方便的识别效果尤为明显.他一般由一个或者多个卷积层组成，相比较其他深度、前馈神经网路，卷积神经网路需要考量的参数更少，使之成为一种颇具吸引力的深度学习结构 TODO]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习算法</tag>
        <tag>cnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习(一)神经网络]]></title>
    <url>%2F2017%2F09%2F27%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。 TODO]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>深度学习算法</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(八)随机森林]]></title>
    <url>%2F2017%2F09%2F27%2F%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[随机森林是表示决策树总体的一个专有名词，在随机森林算法中，我们有一系列的决策树（因此又名“森林”），为了根据一个新对象的属性将其分类，每一个决策树有一个分类，称之为这个决策树“投票”给该分类，这个森林选择获得森林里（在所有树中）获得票数最多的分类. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(七)决策树]]></title>
    <url>%2F2017%2F09%2F27%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树算法是一种监督式的学习算法，我们通过属性相关度不断的将总体分成两个或者多个类别，最终完成决策的不同判别. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(六)朴素贝叶斯]]></title>
    <url>%2F2017%2F09%2F27%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯算法主要的理论依据是贝叶斯公式，利用事件的先验概率计算后验概率.而朴素贝叶斯则是使用了一种类似的方法，分析不同的属性，预测其不同类别的概率.朴素贝叶斯模型非常简单直观，同时对于大型的数据集，往往具有出乎意料的表现. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(五)支持向量机]]></title>
    <url>%2F2017%2F09%2F27%2Fsvm%2F</url>
    <content type="text"><![CDATA[支持向量机(SupportVector Machines)是机器学习中极其重要的算法，在深度神经网络大红大紫之前，svm算法在模式识别领域几乎一统天下.支持向量机的基本思想非常简单，在一组给定的包含正样本和负样本的数据集中，它的目的就是寻找一个超平面将正负样本分开.理论上无论多么复杂的样本集，都存在一个高维空间，其正负样本是线性可分的. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>svm</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(四)k-means聚类]]></title>
    <url>%2F2017%2F09%2F27%2Fk-means%2F</url>
    <content type="text"><![CDATA[k-means是一种最简单的无监督学习算法，它主要解决聚类问题.同一类的数据集本质上具有相似性，而且能显著的区别于其他类别.k-means通过不断调整聚类中心，将一定距离内或者相似性的样本聚为一类. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>聚类</tag>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(三)KNN算法]]></title>
    <url>%2F2017%2F09%2F27%2Fknn%2F</url>
    <content type="text"><![CDATA[KNN(k-Nearest Neighbor)算法是一种最简单直观的机器学习分类算法.它通过计算不同特征值之间的距离来对数据集进行分类. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>KNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(二)逻辑回归]]></title>
    <url>%2F2017%2F09%2F27%2Flogistic%2F</url>
    <content type="text"><![CDATA[逻辑回归回归和线性回归的主要区别在于，逻辑回归可以进行回归预测，但最主要的应用还是二分类，逻辑回归本质上一个被logistic函数归一化后的线性回归，线性回归的输出范围在正无穷到负无穷之间，而逻辑回归的输出范围被压缩到0-1之间. TODO]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(一)线性回归]]></title>
    <url>%2F2015%2F08%2F18%2Flinear%2F</url>
    <content type="text"><![CDATA[介绍 线性回归是机器学习中的重要算法之一，其主要目的就是建立一个线性方程来预测目标值.线性回归所求解的是回归方程的回归系数. 举个例子，我们现在有一组数据如下所示： \(x\) 3.30 4.40 5.50 6.71 6.93 4.17 9.78 6.18 7.59 2.17 7.04 10.79 5.31 7.99 5.65 9.27 3.10 \(y\) 1.70 2.76 2.09 3.19 1.69 1.58 3.37 2.60 2.53 1.22 2.83 3.47 1.65 2.90 2.42 2.94 1.30 其原始和归一化后的数据分布图如下所示： 线性回归 线性回归的一般问题就是，针对给出的数据，拟合出一个能够较为准确预测出输出结果的线性模型，模型一般如下： \[ f(x) = wx+b \tag {1} \] \[J(w,b) = \frac{1}{2}\sum_{i=1}^n(y_i-f(x_i))^2 = \frac{1}{2}\sum_{i=1}^n(y_i-wx_i-b)^2 \tag{2}\] 上式中: \(f(x)\)是预测值 \(y_i\)是真实值 \(J(w,b)\)是代价函数 \(x_i\)是输入值 \(w\)和\(b\)是回归方程需要求解的参数 \(J(w,b)\)表示了预测结果和真实结果之间的误差，\(J(w,b)\)越小表明预测结果越接近真实结果，因此我们希望找到一组\(w\)和\(b\)使得\(J(w,b)\)能够最小，因此对\(w\)，\(b\)的求解问题就变成了一个如下所示的泛函能量最小化的问题： \[ \hat{(w,b)} = \mathop{\arg\min}\limits_{w,b}(J(w,b)) \tag{3} \] 根据公式\({2}\)分别求\(J(w,b)\)关于\(w\)和\(b\)的偏导数： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{4} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) \tag{5} \] 最小二乘法 对于公式\({(3)}\)中\(J(w,b)\)的最小化问题，我们可以将其看作是自变量为\(w\),\(b\),因变量为\(J(w,b)\)的函数，这样最小化的问题就变成了求\(J(w,b)\)极值的问题，根据数学知识我们知道，函数的极值点为偏导数为\(0\)的点，因此： \[ \frac{\partial J(w,b)}{\partial w} = -\sum_{i=1}^nx_i(y_i-wx_i-b) = 0 \tag{6} \] \[ \frac{\partial J(w,b)}{\partial b} = -\sum_{i=1}^n(y_i-wx_i-b) = 0 \tag{7} \] 求解二元二次方程组得到： \[ w = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2} \tag{8} \] \[ b = \mathop{y}\limits^--w\mathop{x}\limits^- \tag{9} \] 上式中： \(\mathop{x}\limits^-=\frac{1}{n}\sum_{i=1}^nx_i\):表示\(x\)的均值 \(\mathop{y}\limits^-=\frac{1}{n}\sum_{i=1}^ny_i\):表示\(y\)的均值 根据样本的方差和协方差公式我们有： \[ var(x) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)^2}{n-1} \tag{10}\] \[ cov(x,y) = \frac{\sum_{i=1}^n(x_i-\mathop{x}\limits^-)(y_i-\mathop{y}\limits^-)}{n-1} \tag{11}\] 方差\(var(x)\)是用来衡量样本分散程度的，方差越小，样本越集中；方差越大，样本越分散 而协方差是用来衡量两个变量的线性相关性，协方差为正，说明两个变量是线性正相关；协方差为负，说明两个变量是线性负相关；协方差为0，说明两个变量没有线性相关性. 故式\((8)\)可表示为： \[ w = \frac{conv(x,y)}{var(x)} \tag{12} \] 最小二乘法的优势在于计算简单，快速，并且找到的估计参数是全局极小值；缺点是对于异常值极其敏感. 对上述数据利用最小二乘法拟合得到的曲线： 梯度下降法 梯度下降是一种最优化算法，对一个函数来说，梯度不仅表示某个向量的偏导数，同时还代表了该向量的方向，在这个方向上，函数增加得最快，在相反的方向上，函数减小得最快。因此为了最小化目标函数\(J(w,b)\),我们可以通过不断迭代的方法，使\(w\)和\(b\)沿着梯度下降的方向进行移动，逐渐逼近极小值. \[ w^{\tau+1} := w^\tau - \alpha\frac{\partial J(w,b)}{\partial w} = w^\tau + \alpha\sum_{i=1}^nx_i(y_i-wx_i-b) \tag{13} \] \[ b^{\tau+1} := b^\tau - \alpha\frac{\partial J(w,b)}{\partial b} = b^\tau + \alpha\sum_{i=1}^n(y_i-wx_i-b) \tag{14} \] 上式中 \(\tau\)是迭代次数 \(\alpha\)是学习率即每次参数移动的步长，学习率太大则可能会错过极小值，太小则会导致收敛速度太慢 \(w^\tau\)和\(b^\tau\)是第\(\tau\)次的参数 \(w^{\tau+1}\)和\(b^{\tau+1}\)是第\({\tau+1}\)次的参数 随机梯度下降法 由式\({(13)}\)和\({(14)}\)可知，梯度下降法的每次参数更新，都需要计算整个数据集,因此运算效率是极低的.而随机梯度下降法的每次更新，都是对数据集中的一个样本计算其代价函数，然后求偏导更新模型参数： \[ w^{\tau+1} := w^\tau + {\alpha}x_i(y_i-wx_i-b) \tag{15} \] \[ b^{\tau+1} := b^\tau + {\alpha}(y_i-wx_i-b) \tag{16} \] 小批量梯度下降法 每次只用一个样本对模型参数进行更新会导致迭代过程中模型参数的变化会很剧烈，同时目标函数在寻找极小值的过程中会有较大震荡.小批量梯度下降法的每次更新，都会对数据集中的一小批样本计算其代价函数，然后求偏导更新模型参数. \[ w^{\tau+1} := w^\tau + {\alpha}\sum_{i=1}^mx_i(y_i-wx_i-b) \tag{17} \] \[ b^{\tau+1} := b^\tau + {\alpha}\sum_{i=1}^m(y_i-wx_i-b) \tag{18} \] 其中： \(m{\subset}n\), \(m\)是\(n\)的子集 梯度下降优化算法 梯度下降法中参数更新策略还有一些其他的优化方法，如动量法，Adagrad，Adadelta，Adam等等 对于文章开头的数据利用两种梯度下降法拟合得到的曲线为： 下图为不同梯度下降优化算法的代价函数的变化： 由上图可以看出Adam相对于SGD会收敛的更快，并且代价函数的震荡相对来说更小. 总结 线性回归是机器学习算法中最为基础和简单的，确立了\(方法=模型+策略+算法\)的基本思路 模型，指的是需要拟合的样本空间的概率分布或者决策函数 策略，指的是我们是通过什么准则来学习或者选择最最优模型的，一般监督学习中的两个基本策略是经验风险和结构风险最小化 算法，指的是通过什么方法去最小化经验风险或者结构风险，一般常见的有SGD,Adam等 线性回归中用到的各种概念，方法在其他各种更为复杂的回归方法中都是适用的，每种机器学习算法只不过是或者模型不同，或者是策略不同，或者是算法不同]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习算法</tag>
        <tag>线性回归</tag>
        <tag>梯度下降法</tag>
      </tags>
  </entry>
</search>
